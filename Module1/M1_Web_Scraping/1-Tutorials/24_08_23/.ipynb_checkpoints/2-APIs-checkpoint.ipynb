{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gwwx6YFTV9Vt"
   },
   "source": [
    "# Working with APIs #\n",
    "#### CAS Applied Data Science 2023 ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEa2mx7XBdfK"
   },
   "source": [
    "In the last tutorial, we learned how to automatically retrieve pages from the internet through **web scraping**. We sent HTTP requests with the  ``requests`` and used the ``BeautifulSoup`` library to parse and work with the HTML code from the response we got. For many of the more popular websites such as Wikipedia, Youtube, Twitter (X) or many newspapers, there is a more direct way to retrieve the information we need: through so-called **APIs**. In this tutorial we will learn how to gather web data through APIs. We will use the Wikipedia API to illustrate the process.\n",
    "\n",
    "To follow allong with the tutorial, you will have to import the ``requests`` and the ``BeautifulSoup`` library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L7h726lofAAH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTIe5VIL3feT"
   },
   "source": [
    "## Getting help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfSxfzDpTjRV"
   },
   "source": [
    "We are selective in this tutorial and only discuss elements that we believe are most important for the purpose of this class. If you want more details, you can consult, for example, the **Python Standard Library Reference** at https://docs.python.org/3/library/ or the **Language Reference** at https://docs.python.org/3/reference/. But be warned: the amount of detail in these sources can be overwhelming. For **quick and easy-to-understand overviews** of different topics see, for example, https://www.w3schools.com/python/. Here are some specific references for today's tutorial:\n",
    "\n",
    "Working with APIs in Python:\n",
    "* https://realpython.com/python-api/\n",
    "* https://www.dataquest.io/blog/python-api-tutorial/\n",
    "\n",
    "JSON:\n",
    "*  https://www.w3schools.com/js/js_json_intro.asp\n",
    "\n",
    "If you get stuck or don't remember how to do something, it is usually a good idea to **Google** your problem. Python has a large (and fast-growing) community and you will probably find answers to most of your questions online (e.g. on **Stack Overflow** or in a **Youtube tutorial**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTTxqag08CcA"
   },
   "source": [
    "## What is an API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFIjdUC8Lsa_"
   },
   "source": [
    "Suppose you have a (large) list of addresses and need the respective coordinates. If you wanted to do this manually, you could use Google Maps through your browser. But how could you automate this? You could try to generate the URLs for each request and fetch the coordinates from the pages. This may be feasable, but the pages you will request contain a lot of information and graphical \"overhead\" you are not interested in, making the process very tedious and inefficient (and web scrapers often get blocked by such services, as they are mostly comercial or have limited capacity). Fortunately, many providers offer an easier way to access the information in their databases: through APIs (e.g. the Google Maps API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLf9nocWQGq_"
   },
   "source": [
    "So what is an API? API stands for **Application Programming Interface** and it is a very broad concept. You can think of it as an interface that is not meant for humans but for computer programs or applications. APIs allow different applications or programs to interact with each other. For example, the different programs installed on your computer may communicate with the operating system of your computer through APIs (e.g. when saving a file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNcOUGixDxPU"
   },
   "source": [
    "  \n",
    "In our context of gathering data from the web, we are mostly dealing with so-called **Web APIs**. They allow you to access content in a more **structured** way than scraping HTML pages. Web APIs consist of **a set of URLs through which the data is made accessible so that other applications/programs (for example your Python script) can work with it conveniently**. They also allow to take more control over the content you will receive. When providers offer an API, it just means that they’ve built a set of URLs that return structured data — meaning the responses won’t contain the presentational overhead that is required for a graphical user interface like a website. Web APIs have become so important that many people just say API when they are referring to a Web API.\n",
    "\n",
    "This probably still sounds very abstract. We'll walk you through some examples with the Wikipedia API to give you a more concrete idea of how APIs work. That being said, we still download and process data via an URL, only the URL and data structure will change a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb7JJyVLCNVo"
   },
   "source": [
    "## Making API requests with the ``requests`` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKXhsS2cdG8u"
   },
   "source": [
    "How can we gather data through an API? In practice, **retrieving data through an API is quite similar to retrieving data through web scraping**. You will also have to **figure out the correct URL** for your request. In fact, you can type this URL into the browser — **instead of a designed web page, you will directly see structured data**. For example, this URL takes you to the structured data from the cat article on Wikipedia: https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neIjGVGNf9hq"
   },
   "source": [
    "If we want to request the cat article through the API, we can thus just type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6O2xXujdgIEx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json\")\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10iYQsKhiZUK"
   },
   "source": [
    "Let's take a closer look at the URL we used: The first part of the URL (https://en.wikipedia.org/w/api.php) is an **endpoint** of the API. You can think of endpoints as the base URLs of the API. After the ``?`` we have the **query string** (as we know already from the web scraping tutorial). It specifies the action we want to perform. Usually, it is more convenient to enter the parameters for the query string as an argument. The **``params`` parameter of the ``get`` function** allows you to do this (the order of the parameters typically does not matter):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2Zer5lYNk4P-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"page\": \"Cat\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "r = requests.get(url=ENDPOINT, params=PARAMS)\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzoci3rgmrWe"
   },
   "source": [
    "To see that this makes exactly the same request, you can take a look the URL we just called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "I5Z6Zs-vmwX_",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1jFXiM_gZb5"
   },
   "source": [
    "So far, so good. But how did we figure out that https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json was the URL we needed to request? Many APIs have an extensive **documentation** that explain how to make different types of requests. These documentations can sometimes be overwhelming and it is usually a good idea to try to find some examples for the type of request you want to make (e.g. on Stack Overflow or within the documentation). Let's have a look at the documentation for the Wikipedia API:\n",
    "\n",
    "* https://www.mediawiki.org/wiki/API:Main_page\n",
    "\n",
    "We can see that there are different endpoints. For example, there is one endpoint for each language. The endpoint for English Wikipedia is:\n",
    "* https://en.wikipedia.org/w/api.php\n",
    "\n",
    "Now we will have to figure out what parameters to choose (i.e. how to construct the query string). If you scroll down on the page, you will see that there are many options as to how the parameters could be set. If you click through the documentation (or Google something like \"parse page Wikipedia API\") you may eventually get to this page that provides some useful examples as to how the content of a page could be retrieved:\n",
    "* https://www.mediawiki.org/wiki/API:Parsing_wikitext\n",
    "\n",
    "There, you will find example code similar to the one we used above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmxIC_0odgEI"
   },
   "source": [
    "><font color = 4e1585> SIDENOTE: APIs often allow you to do many different things. You can retrieve information, but sometimes you can also add, change or delete information. Many Web APIs are so-called RESTful APIs, meaning that they implement the standard HTTP methods: GET, POST, PUT, and DELETE. For data science projects, you are typically only interested in GET requests (as you don't want to change anything on the respective websites). </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq5A6gPV8HAu"
   },
   "source": [
    "## Working with JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrES187T2RDH"
   },
   "source": [
    "We mentioned that APIs allow you to access *structured data*. What exactly does this mean? Typically, the data you will retrieve through an API will be formatted as **JSON**. JSON stands for JavaScript Object Notation; it is a data exchange format based on human-readable text (as opposed to binary data formats) that is commonly used for data transportation over the web. Let's take a look at some JSON code:\n",
    "\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"Peter\",\n",
    "  \"age\": 56,\n",
    "  \"married\": false,\n",
    "  \"children\": [\n",
    "    {\n",
    "      \"name\": \"Nina\",\n",
    "      \"age\": 19,\n",
    "      \"educational degree\": \"High school\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"Mary\",\n",
    "      \"age\": 14,\n",
    "      \"educational degree\": null\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej-SXXBEOcD3"
   },
   "source": [
    "><font color = 4e1585> SIDENOTE: If you want to know if something is valid (correctly formatted) JSON you can search for 'JSON lint' or 'JSON linter' and use one of the online tools, e.g. https://jsonlint.com. A lint or linter is a tool to check for errors in code (see https://en.wikipedia.org/wiki/Lint_(software) for more details)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxWYiHmm3trK"
   },
   "source": [
    "You may have noticed that JSON does not look so different from a Python dictionary. The great thing about JSON is that it can easily be **converted into a (nested) Python dictionary**. Let's parse the response we got from our Wikipida API request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NMUBPFSW3RBJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "I5zeTd8_5mqv",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(data)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ0o49Hs5pU4"
   },
   "source": [
    "We have now created a (large) Python dictionary. How can we **access the different parts** of it? If you **type the URL of your request into your browser**, you can expore the nested structure of the JSON code (and thus of your Python dictionary):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VBFqFa8e-s0e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/w/api.php?action=parse&page=Cat&format=json'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.url # this will display the URL; copy the URL to your browser and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtmMGBVU-z2E"
   },
   "source": [
    "\n",
    "Similarly, it is usually a good idea to explore the **keys** of the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EZyjF7ZR60-6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['parse'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIxXtvYO64fl"
   },
   "source": [
    "We only have one key at the top level (``parse``). Let's explore its value to see what we have on the next level:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "W2x4SwQP7OoF",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'pageid', 'revid', 'text', 'langlinks', 'categories', 'links', 'templates', 'images', 'externallinks', 'sections', 'showtoc', 'parsewarnings', 'displaytitle', 'iwlinks', 'properties'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"parse\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLJ5eGwV7U_5"
   },
   "source": [
    "Now we can access different types of information about the page. Let's access the title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DlqrKJXt7zTR",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"parse\"][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP09G8LT8N0H"
   },
   "source": [
    "Let's now try to access the sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jpKjIZvg79XG",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = data[\"parse\"][\"sections\"]\n",
    "type(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toclevel': 1,\n",
       " 'level': '2',\n",
       " 'line': 'Etymology and naming',\n",
       " 'number': '1',\n",
       " 'index': '1',\n",
       " 'fromtitle': 'Cat',\n",
       " 'byteoffset': 12383,\n",
       " 'anchor': 'Etymology_and_naming',\n",
       " 'linkAnchor': 'Etymology_and_naming'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyDh_DSl8frc"
   },
   "source": [
    "We got a list of dictionaries containing information on each section. If we are only interested in the heading of each section, we could write a list comprehension to extract it. The \"line\" key in each dictionary appears to contain the section heading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IVory0Zl81nj",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Etymology and naming',\n",
       " 'Taxonomy',\n",
       " 'Evolution',\n",
       " 'Domestication',\n",
       " 'Characteristics',\n",
       " 'Size',\n",
       " 'Skeleton',\n",
       " 'Skull',\n",
       " 'Claws',\n",
       " 'Ambulation',\n",
       " 'Balance',\n",
       " 'Coats',\n",
       " 'Senses',\n",
       " 'Vision',\n",
       " 'Hearing',\n",
       " 'Smell',\n",
       " 'Taste',\n",
       " 'Whiskers',\n",
       " 'Behavior',\n",
       " 'Sociability',\n",
       " 'Communication',\n",
       " 'Grooming',\n",
       " 'Fighting',\n",
       " 'Hunting and feeding',\n",
       " 'Play',\n",
       " 'Reproduction',\n",
       " 'Lifespan and health',\n",
       " 'Disease',\n",
       " 'Ecology',\n",
       " 'Habitats',\n",
       " 'Ferality',\n",
       " 'Impact on wildlife',\n",
       " 'Interaction with humans',\n",
       " 'Shows',\n",
       " 'Infection',\n",
       " 'History and mythology',\n",
       " 'Superstitions and rituals',\n",
       " 'See also',\n",
       " 'References',\n",
       " 'External links']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[section[\"line\"] for section in sections]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzEfQFsn9XS3"
   },
   "source": [
    "In this way, you can access different parts of your dictionary and extract the information on the page (e.g. internal and external links, images etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY8B_wXz9Tjl"
   },
   "source": [
    "The dictionary (within \"parse\") **also contains a \"text\" key where the HTML code of the article is stored** (within the \"*\" key). You can convert it to a BeautifulSoup object and search it based on HTML tags (see notebook on web scraping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Do3PxVSl-2-z",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get html text\n",
    "html_text = BeautifulSoup(data[\"parse\"][\"text\"][\"*\"])\n",
    "type(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zouIBKCs_eWA"
   },
   "outputs": [],
   "source": [
    "# get opening paragraph\n",
    "html_text.find_all(\"p\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt07LNudBE8p"
   },
   "source": [
    "---\n",
    "## **<font color='teal'>In-class exercise**\n",
    "\n",
    ">  <font color='teal'> Request the *Dog* page on Wikipedia (formatted as JSON) through the API and convert the response to a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tkq6nK-LzPHF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"page\": \"Dog\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "r_dog = requests.get(url=ENDPOINT, params=PARAMS)\n",
    "#r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dog = r_dog.json()\n",
    "type(data_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBzARxtjzmzZ"
   },
   "source": [
    ">  <font color='teal'> Print the title and all links to other Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0iSEvtl45xC-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'pageid', 'revid', 'text', 'langlinks', 'categories', 'links', 'templates', 'images', 'externallinks', 'sections', 'showtoc', 'parsewarnings', 'displaytitle', 'iwlinks', 'properties'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dog[\"parse\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "luyghogH7T28",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dog'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dog[\"parse\"][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "WQHq1uQK8mCw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ns': 0, 'exists': '', '*': 'Dog (disambiguation)'},\n",
       " {'ns': 0, 'exists': '', '*': 'Doggy (disambiguation)'},\n",
       " {'ns': 0, 'exists': '', '*': 'Pooch (disambiguation)'},\n",
       " {'ns': 0, 'exists': '', '*': 'Doggy'},\n",
       " {'ns': 0, 'exists': '', '*': 'Pooch'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_dog = data_dog[\"parse\"][\"links\"]\n",
    "links_dog[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows just the Wikipedia name of the webpage linked, because the base URL before e.g. 'Dog (disambiguation)' is the same for all internal wikipedia links. \n",
    "We can call just the link names into a separate list if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dog (disambiguation)',\n",
       " 'Doggy (disambiguation)',\n",
       " 'Pooch (disambiguation)',\n",
       " 'Doggy',\n",
       " 'Pooch',\n",
       " 'Canis lupus dingo',\n",
       " 'Evolution of the wolf',\n",
       " 'Domestication of the dog']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_name = [link[\"*\"] for link in links_dog]\n",
    "#type(link_name)\n",
    "link_name[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are also other links in the data_dog dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "sflRPDT_8eJ-",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[list, 13]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwlinks_dog = data_dog[\"parse\"][\"iwlinks\"]\n",
    "test = [0,0]\n",
    "test[0] = type(iwlinks_dog)\n",
    "test[1] = len(iwlinks_dog)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prefix': 'de',\n",
       "  'url': 'https://de.wikipedia.org/wiki/Kesslerloch',\n",
       "  '*': 'de:Kesslerloch'},\n",
       " {'prefix': 'wikt',\n",
       "  'url': 'https://en.wiktionary.org/wiki/dog',\n",
       "  '*': 'wikt:dog'},\n",
       " {'prefix': 'c',\n",
       "  'url': 'https://commons.wikimedia.org/wiki/Dog',\n",
       "  '*': 'c:Dog'},\n",
       " {'prefix': 'n',\n",
       "  'url': 'https://en.wikinews.org/wiki/Special:Search/Dog',\n",
       "  '*': 'n:Special:Search/Dog'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwlinks_dog[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://doi.org/10.1007%2F13836_2018_27',\n",
       " 'https://www.biodiversitylibrary.org/item/80764#page/48/mode/1up',\n",
       " 'http://www.departments.bucknell.edu/biology/resources/msw3/browse.asp?id=14000751',\n",
       " 'http://www.google.com/books?id=JgAMbNSt8ikC&pg=PA575%E2%80%93577']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "externallinks_dog = data_dog[\"parse\"][\"externallinks\"]\n",
    "externallinks_dog[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEMlhE3PzPeW"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqoiHSlKTADW"
   },
   "source": [
    "## Making advanced requests with the Wikipedia API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYnRJp_P_92w"
   },
   "source": [
    "The Wikipedia API allows you to do much more than just retrieving a single page. For example, you can search for **pages that match with certain words** (full text search in title or content: https://www.mediawiki.org/wiki/API:Search or https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bsearch). Let's retrieve all pages about cats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "5KK6cH0RFq4r",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform page search\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"list\": \"search\",\n",
    "    \"srsearch\": \"cat\",\n",
    "    \"srlimit\": \"max\"\n",
    "}\n",
    "\n",
    "r = requests.get(url=URL, params=PARAMS)\n",
    "\n",
    "# Convert to dictionary\n",
    "DATA = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "lYQ4zUrAyBcF",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batchcomplete', 'continue', 'limits', 'query'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "eA0Eui7wGT9Z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batchcomplete', 'continue', 'limits', 'query'])\n",
      "dict_keys(['searchinfo', 'search'])\n"
     ]
    }
   ],
   "source": [
    "# Navigate through dictionary\n",
    "print(DATA.keys())\n",
    "print(DATA[\"query\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "oT99YJukF1EV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat', 'Cat (disambiguation)', 'Çat', '.cat', 'Persian cat', \"Schrödinger's cat\", 'Cat Stevens', 'Sphynx cat', 'Black cat', 'Bengal cat', 'Pussy Cat Pussy Cat', 'Calico cat', 'Cat the Cat', 'Miraculous: Tales of Ladybug & Cat Noir', 'Cat-sìth', 'Tabby cat', 'Leopard cat', 'Maine Coon', 'Felidae', 'Siamese cat', 'Doja Cat', 'Fishing cat', 'Bombay cat', 'Black Cat, White Cat', 'CatCat', 'Ladybug & Cat Noir: The Movie', 'Munchkin cat', 'Cat meat', 'Tortoiseshell cat', 'The Cat', 'Cats and the Internet', 'Cat behavior', 'Siberian cat', 'Sam & Cat', 'Feral cat', 'Manx cat', 'Cat anatomy', 'Cat communication', 'Sand cat', 'Bicolor cat', 'Herding cats', 'Cat (Unix)', 'Cat People', 'Farm cat', 'Savannah cat', \"Pallas's cat\", 'Aging in cats', 'Polydactyl cat', 'WorldCat', 'Jungle cat', 'Domestic short-haired cat', 'Nyan Cat', 'Cats (musical)', 'Cat senses', 'Category 6 cable', 'Cat Cora', 'Domestication of the cat', 'Cat House', 'Cat intelligence', 'Allergy to cats', 'Alley cat', 'Kneading (cats)', 'List of oldest cats', 'Cheshire Cat', \"Cat o' nine tails\", \"Cat's paw\", 'Lolcat', 'Big cat', 'Cat Valentine', 'Civet cat', 'Çatal', 'Himalayan cat', 'Nature Cat', 'Scottish Fold', 'Cát Hải district', 'Keyboard Cat', 'Ragamuffin cat', 'Top Cat', 'Cool Cat', 'Cat Deeley', 'Aegean cat', 'Category 5 cable', 'List of cat breeds', 'Cat7', 'Lykoi', 'The Cats', 'Birman', 'Human interaction with cats', 'Scarlet (Doja Cat album)', 'Abyssinian cat', 'Skane Jet', 'Grumpy Cat', 'Ora Lightning Cat', 'British Shorthair', 'Cats in ancient Egypt', 'Donskoy cat', 'Kellas cat', 'Cat (comics)', 'Bay cat', 'Cat grass', 'Cat on a Hot Tin Roof (1958 film)', 'Puppy cat', 'Cat training', 'Asian golden cat', 'Category 2 cable', 'Ragdoll', 'Oncilla', \"Ship's cat\", 'Cat flea', 'Ringtail', 'Cat food', 'My Cat from Hell', 'Ora Good Cat', 'Cat café', 'Cat Power', 'Black Cat (Marvel Comics)', 'The Cat and the Fiddle', 'Bodega cat', 'That Darn Cat!', 'Cat fugue', 'Rusty-spotted cat', 'Serengeti cat', 'Odd-eyed cat', 'Somali cat', 'Cats in the United States', 'CAT III', 'Larry (cat)', 'Leopardus narinensis', 'Willow (cat)', 'Van cat', 'Cat righting reflex', 'Domestic long-haired cat', 'Hellcat', 'Cat culture', 'Cat3', 'Iriomote cat', 'Bad Cat', 'Stray Cats', 'Moz & Cat', 'Burmese cat', 'Dwarf cat', 'Cát Bà Island', 'Cat Ballou', 'The Cat and the Canary', 'International Cat Day', 'The Cat Returns', 'Cat (zodiac)', 'Dead cat', 'Snowshoe cat', 'Black cat (disambiguation)', 'Sylvester the Cat', 'Highland cat', 'The Cat in the Hat', 'Creme Puff (cat)', 'ThunderCats', \"Simon's Cat\", 'Cat eye glasses', 'White Cat', 'Highlander cat', 'Cat and mouse', 'CatDog', 'Cator', 'Cat coat genetics', 'I Am a Cat', 'Cats and Dogs', 'Cat Marnell', 'Arctic Cat', 'Garfield (character)', 'Norwegian Forest cat', 'Grumman Ag Cat', 'Cat lady', 'Old cat', 'Adrien Agreste', 'Mad Cat', 'Ora Ballet Cat', 'American Shorthair', 'Pork–cat syndrome', 'LTE-M', 'Year of the Cat', 'Super Cat', 'Phantom cat', 'Catfish', 'Cat tapeworm', 'Tom Cat', 'Cat-scratch disease', 'Cats, Cats, Cats!', 'Attention (Doja Cat song)', 'Singapura cat', 'Feral cats in Istanbul', 'Ćatić', 'List of Miraculous: Tales of Ladybug & Cat Noir episodes', 'List of Nature Cat episodes', 'Cat–dog relationship', 'Cat Burns', 'Cat Girl', 'Planet Her', 'Felid hybrids', 'Chinese mountain cat', 'European cat snake', 'Cat Island', 'Cat on a Hot Tin Roof', 'Stubbs (cat)', 'The Black Cat (short story)', 'Tip-cat', 'Common Admission Test', 'Stalking Cat', 'Cat organ', \"Cat's eye\", \"Geoffroy's cat\", 'Tonkinese cat', 'T.H.E. Cat', 'The Cat Concerto', 'Machairodontinae', 'Golden cat', 'Meow Mix', 'Cat Cays', 'Catgirl', 'Turkish Angora', 'Black-footed cat', 'The Head Cat', 'Kitten', 'Cashmere Cat', 'Curiosity killed the cat', 'Fat cat', 'The Masterful Cat Is Depressed Again Today', 'Maneki-neko', 'Pussycat', 'Tomcat', 'Cyprus cat', 'List of experimental cat breeds', 'Cat in a box', 'Andean mountain cat', 'Buttered cat paradox', 'Meow (cat)', 'Islam and cats', 'Felix the Cat', 'List of cat body-type mutations', 'Maru (cat)', 'National Cat Day', 'Cat registry', 'The Cassandra Cat', \"The Minister's Cat\", 'Jellicle cats', 'Wampus cat', 'That Darn Cat (1997 film)', 'Morph the Cat', 'Cat House, Riga', 'Cat gap', 'Cat tongue', 'Marbled cat', 'The Amazing Acro-Cats', 'Winged cat', 'Doja Cat discography', 'Litter box', 'Amala (album)', 'Cat predation on wildlife', 'Ghost Cat', 'Cat Burglar', 'Diabetes in cats', 'CAT (2022 TV series)', 'Big Cat Diary', 'Cates', 'Eek! The Cat', 'Mountain cat', 'Big Cat, Little Cat', 'Dorothy Gambrell', 'Leopold the Cat', 'Cat Island (Mississippi)', 'Thai cat', 'Cat repeller', 'Boiga ceylonensis', 'Egyptian Mau', 'Black Cat (Washington, D.C., nightclub)', 'Pampas cat', 'Cat pheromone', 'Year of the Cat (song)', 'Russian Blue', 'Cat bell', 'Peg + Cat', 'Cat People (1982 film)', 'Australian Mist', 'Tommy the Cat', 'Cat Stevens discography', 'Boiga siamensis', 'Cat Bi International Airport', 'Selkirk Rex', 'Felix the Cat filmography', 'Cat burning', 'The Cat in the Hat (film)', 'Smelly Cat', 'Lynx', 'Letting the cat out of the bag', 'Congenital sensorineural deafness in cats', 'Belling the Cat', 'Bến Cát', 'Cathead', 'Kit-Cat Klock', 'The Cat Came Back', 'African wildcat', 'Bambino cat', 'List of feline diseases', 'The Cat Who...', 'Cat People (1942 film)', 'Ailurophobia', 'Peter', 'Cat state', 'CC (cat)', 'A Cat in Paris', 'Battle Cat', 'Boiga', 'Cat show', 'Cat tree', 'Catastrophe modeling', 'Hot Pink (album)', 'Zoom Cat Lawyer', 'List of individual cats', 'Cats in New Zealand', 'List of Top Cat characters', 'Cát Tiên archaeological site', 'My Cat Likes to Hide in Boxes', 'Felis margarita thinobia', 'Boiga multomaculata', 'Cat Zingano', 'Cats (disambiguation)', 'Lasagna Cat', 'The Battle Cats', 'Icelandic Christmas folklore', 'Cat eye syndrome', 'Cat bite', 'Black Cat Detective', 'Cat Island, Bahamas', 'The Cat Empire', 'British big cats', 'Cats in Australia', 'Cat health', 'Kit Kat (disambiguation)', 'The Fox and the Cat', 'CAT II', \"Schrödinger's cat in popular culture\", 'Cat City', 'European wildcat', 'P.S. Your Cat Is Dead (film)', 'Socks (cat)', 'Cat phone', 'Phu Cat Airport', 'CT scan', 'Jones (fictional cat)', 'Nora (cat)', 'Felis lunensis', 'Perth Central Area Transit', 'Cats & Dogs', 'Catnip', 'Cat Claw', 'Pete the Cat', 'Cat Person', 'Fritz the Cat', 'Smudge', 'Blake Snyder', 'Category 3 cable', 'CAT(k) space', 'Minuet cat', 'Tama (cat)', \"Henry's Cat\", 'Cabbit', 'False saber-toothed cat', 'American Bobtail', 'Hobie Cat', 'Cat Palmer', 'Hamilton Tiger-Cats', \"Cat Fanciers' Association\", 'Purrr!', 'Phoebe Cates', 'Cat Spring, Texas', 'The Mansion Cat', 'List of cat documentaries, television series and cartoons', 'Cat people and dog people', 'Browser (cat)', 'Pixie-bob', 'Cat Person (film)', 'Cat Hill, Ascension Island', 'Cat Soup', 'Cymric cat', 'Asian cat', 'Wey Macchiato', 'Cat Fancy', 'Banded cat-eyed snake', 'Working cat', 'Cat Herders', 'Werecat', 'James the Cat', 'Mud cat', 'The Cat in the Hat Knows a Lot About That!', 'The Cat of Bubastes', 'Kick the cat', 'Macavity', 'The Curse of the Cat People', 'Feline leukemia virus', 'Ora Black Cat', 'Korat', 'Copycat', 'Stampy', 'Black Cat (song)', 'The Aristocats', 'Cat Quest II', 'Cat Shit One', 'American Curl', 'Morris the Cat', 'Ion', 'Gus: The Theatre Cat', 'Olivia Benson (cat)', 'Cat Sandion', 'Leopardus guttulus', 'Fritz the Cat (film)', 'Cactus cat', 'Cat (Red Dwarf)', 'Catwoman', 'Cat Bells', 'The Shadow of the Cat', 'Balinese cat', 'Cat Mario', 'Houtong Cat Village', 'Visayan leopard cat', 'The Family Cat', 'Bombalurina (cat)', 'Cat Cubie', 'The \"Hentai\" Prince and the Stony Cat', 'Polecat', 'Dog and Cat', 'The Catillac Cats', 'Kingdom of Cat', 'Cheshire Cat (disambiguation)', 'Pussy', 'An Cat Dubh / Into the Heart', 'Catmando', 'Library cat', 'Catwoman (disambiguation)', 'Tom and Jerry filmography', 'Coital alignment technique', 'The Owl and the Pussy-Cat', 'Cat Daddy Games', 'A Talking Cat!?!', 'Instrument landing system', 'CAT I', \"Cat's in the Cradle\", 'Corsican wildcat', 'Cat play and toys', 'Oriental Shorthair', 'Wildcat (disambiguation)', 'Havana Brown', 'A Street Cat Named Bob (film)', 'Anarchist symbolism', 'Javanese cat', 'Snow Cat', 'Fisher cat (disambiguation)', 'Simon (cat)', 'Jorts (cat)', 'Boss Bitch', 'Felinae', 'Eating the Cheshire Cat', 'James Bowen (author)', 'Cool Cat (Looney Tunes)', 'Squitten', 'Flat-headed cat', 'The Chinese Siamese Cat', 'African golden cat', 'Kit-Cat Club', \"Cat's claw\", 'Raining cats and dogs', 'Popular cat names', 'Cat-Women of the Moon', 'Black Cat (manga)', 'Smooth Big Cat', 'Asian palm civet', 'Caterpillar C175', 'Margay', 'Mittens (cat)', 'Cat Run', 'Maltese cat', 'Sokoke', 'Catboat', 'Cat Grant', 'Kaibyō', 'All Purpose Cultural Cat Girl Nuku Nuku', 'Cats on the Roof', 'Woman yelling at a cat', 'Year of the Cat (album)', 'Cat Barber', 'CueCat', 'Devon Rex', 'Concert: The Cure Live', 'The Grinch Grinches the Cat in the Hat', 'Marinette Dupain-Cheng', 'Big Cat Rescue', 'Cat gecko', 'Novation CAT', 'Cat Country', 'List of fictional felines', 'Toxoplasmosis', 'Fat Cat and Friends', 'The Everyday Tales of a Cat God']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get titles of all cat pages that were found\n",
    "pages = DATA[\"query\"][\"search\"]\n",
    "cat_pages = [page[\"title\"] for page in pages]\n",
    "print(cat_pages)\n",
    "len(cat_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1v3xdNu5YZR"
   },
   "source": [
    "Another interesting thing you can do is to **find pages based on coordinate locations** (see: https://www.mediawiki.org/wiki/API:Geosearch). Let's find all Wikipedia pages of places that are close (<1km) to the main building of University of Bern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "6BsM2oXTKZz5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform page search\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"  # Change to German Wikipedia!\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"list\": \"geosearch\",\n",
    "    \"gscoord\": \"46.950519|7.438109\",\n",
    "    \"gslimit\": \"max\",\n",
    "    \"gsradius\": 1000\n",
    "}\n",
    "\n",
    "r = requests.get(url=URL, params=PARAMS)\n",
    "\n",
    "# Convert to dictionary\n",
    "DATA = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "A12JL1sEMtkT",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['University of Bern',\n",
       " 'Bern railway station',\n",
       " 'Swissmedic',\n",
       " 'Christoffelturm',\n",
       " 'BX Swiss',\n",
       " 'Bahnhofplatz',\n",
       " 'Kulturzentrum Reitschule',\n",
       " 'Church of the Holy Ghost, Bern',\n",
       " 'Bubenbergplatz',\n",
       " 'Trams in Lausanne',\n",
       " 'Trams in Bern',\n",
       " 'Aarbergergasse',\n",
       " 'Museum of Fine Arts Bern',\n",
       " 'Pfeiferbrunnen',\n",
       " 'Spitalgasse',\n",
       " 'Neuengasse',\n",
       " 'Schauplatzgasse',\n",
       " 'Waisenhausplatz',\n",
       " 'Lorraine railway viaduct',\n",
       " 'Presence Switzerland',\n",
       " 'Holländerturm',\n",
       " 'Käfigturm',\n",
       " 'Bärenplatz',\n",
       " 'Federal Department of Finance',\n",
       " 'Berner Kantonalbank',\n",
       " 'Anna-Seiler-Brunnen',\n",
       " 'Bern Botanical Garden',\n",
       " 'Swiss Agency for Development and Cooperation',\n",
       " 'Swiss Alcohol Board',\n",
       " 'Bundesplatz',\n",
       " 'Zeughausgasse',\n",
       " 'Federal Palace of Switzerland',\n",
       " 'Marzili Funicular',\n",
       " \"St. Paul's Church, Bern\",\n",
       " 'Bern Theatre',\n",
       " 'Kindlifresserbrunnen',\n",
       " 'Kornhausplatz',\n",
       " 'Marktgasse',\n",
       " 'Amthausgasse',\n",
       " 'Bern',\n",
       " 'Hotel Bellevue Palace',\n",
       " 'Zytglogge',\n",
       " 'Kochergasse',\n",
       " 'Burgerbibliothek of Berne',\n",
       " 'Zähringerbrunnen',\n",
       " 'Gurtenfestival',\n",
       " 'Radio RaBe',\n",
       " 'Rathausgasse',\n",
       " 'Herrengasse (Bern)',\n",
       " 'Herrengasse 23 (Bern)',\n",
       " 'Einsteinhaus',\n",
       " 'Old City (Bern)',\n",
       " 'Kramgasse',\n",
       " 'Simsonbrunnen']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLACES = DATA['query']['geosearch']\n",
    "[place[\"title\"] for place in PLACES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVUHkdRcNsYF"
   },
   "source": [
    "There are many more things you can do with the Wikipedia API. Some of them would be extremely difficult (or impossible) to achieve through web scraping (e.g. the two examples in this section). Check out the documentation (or Google your idea) to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI2tCnawRagq"
   },
   "source": [
    "## Sidenote: Using the ``wikipedia`` module for simple requests*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FxBzABtOAzL"
   },
   "source": [
    "<font color='gray'>This section is for self-study and will not be discussed in class.\n",
    "\n",
    "For many of the more popular APIs, someone has written a **Python Module** that allows you to work with the API more conveniently. For example, there is a **``wikipedia`` module** to work with the Wikipedia API (see here for a documentation: https://pypi.org/project/wikipedia/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "OMTcRwNZPCmv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/pbecerra/.pyenv/versions/anaconda3-2023.07-2/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=cc381d2e0d288fdcc26f9d4246663be26d149d86e488c9e491a4b4f93f75702c\n",
      "  Stored in directory: /Users/pbecerra/Library/Caches/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# install and import wikipedia module\n",
    "!pip install wikipedia # When ! is being added at the beginning the statement is performed in the command-line!\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nu5ryId6PF2z",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cat',\n",
       " 'Cat (disambiguation)',\n",
       " 'Çat',\n",
       " '.cat',\n",
       " 'Persian cat',\n",
       " \"Schrödinger's cat\",\n",
       " 'Cat Stevens',\n",
       " 'Sphynx cat',\n",
       " 'Black cat',\n",
       " 'Bengal cat']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for pages about cats\n",
    "wikipedia.search(\"Cat\") # only first 10 results by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "cqrf87_VQagL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get contents of cat page\n",
    "wikipedia.page('Cat', auto_suggest=False).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSPOb7MwRLCk"
   },
   "source": [
    "Such modules can be very convenient if you want to perform simple requests. However, they are often not very flexible and may not always allow you to make the requests you would like to perform. <font color='red'>Moreover, the quality may vary (anyone can program and upload such a module) as the following example shows:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFmkv4fYiAVW"
   },
   "outputs": [],
   "source": [
    "#wikipedia.page('Cat') # Search for 'Cat' shows result for 'Can', as auto_suggest parameter is True by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2mB0tC5WLfU"
   },
   "source": [
    "---\n",
    "## **<font color='teal'>In-class exercise\\***\n",
    "\n",
    "<font color='gray'>This exercise is for self-study and will not be discussed in class.\n",
    "\n",
    ">  <font color='teal'> Retrieve 20 Wikipedia pages that are located within a 2 km radius from the main train station in Bern (46.949722°, 7.439444°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zen6VFiWP1F"
   },
   "outputs": [],
   "source": [
    "# Perform page search\n",
    "\n",
    "\n",
    "# Convert to dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8KlRXWR1ZTH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVS0_l9mTUq8"
   },
   "source": [
    ">  <font color='teal'> How far is each place from the train station? Create a dictionnary (or a pandas DataFrame) that shows the distances for all the pages you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu_YP6q0S-h_"
   },
   "outputs": [],
   "source": [
    "# dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILZ32EycSr0w"
   },
   "outputs": [],
   "source": [
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q69EkTyHdTE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUEJZTQ4WQ3N"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85yXHaH-X6Lx"
   },
   "source": [
    "## Working with APIs in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URLakgjKUJA-"
   },
   "source": [
    "Now we have seen many different ways to retrieve data from the web and you might be asking yourself: When should I do web scraping? When should I use an API? When should I use a Python module (that accesses an API)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlawQ0stVXQw"
   },
   "source": [
    "Or course, this depends on your project and on your preferences, but here's a rule of thumb:\n",
    "\n",
    "* Whenever there is an **API**, it is usually easier to work with it than to scrape the pages. Also, some data is only really accessible through APIs (e.g. if you want to retrieve all tweets about a certain topic on Twitter). So if there's an API, it's usually (but of course not always) a good idea to use it!\n",
    "\n",
    "* If there's a **Python library** for the API, it is a good idea to check it out and see what it can do. If it suits your needs, it may save you a lot of work (reading the API documentation etc.). However, some people prefer to work directly with the API to keep full control over their implementation.\n",
    "\n",
    "* **Web scraping** is useful when there is no API, which is the case for most websites. For some tasks, you may also find it easier to just scrape the pages instead of trying to find out how the API works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_86BDYZ2A3z6"
   },
   "source": [
    "When you start working with other APIs you will notice that each one works a bit differently. For example, many APIs require **authentication** (e.g. Twitter API, Google Maps API, Facebook API etc.). This means that you will first have to create a \"developer account\" and will then get some credentials you have to include into your request. You will have to read the documentation of the API (or some blog or answer on Stack Overflow) to find out how this works for the API you are interested in. Also, not all APIs give you **free access** to all the data. For example, the Google Maps API will start to charge you a price after a certain amount of requests, so be careful with your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sESJcMRBqd1V"
   },
   "source": [
    "APIs are becoming increasingly common and larger providers usually have one (or several). Here are some **APIs you may find useful for future research projects**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9mCCAFysbM7"
   },
   "source": [
    "| Name | Link | Python Module(s) |\n",
    "| --- | --- | --- |\n",
    "| Wikipedia API | https://www.mediawiki.org/wiki/API:Main_page | wikipedia, Wikipedia-API |\n",
    "| Twitter API | https://developer.twitter.com/en/docs/twitter-api | twitter, tweepy |\n",
    " Google Maps API| https://developers.google.com/maps/documentation | python-gmaps|\n",
    "| LinkedIn API|https://www.linkedin.com/developers/|python-linkedin-v2|\n",
    "| New York Times API|https://developer.nytimes.com/apis| pynytimes|\n",
    "| The Guardian API |https://open-platform.theguardian.com/documentation/||\n",
    "| Google Trends (no official API) | https://towardsdatascience.com/google-trends-api-for-python-a84bc25db88f | pytrends|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R-2C_2yVDvn"
   },
   "source": [
    "---\n",
    "## **<font color='teal'>In-class exercise\\***\n",
    "\n",
    "<font color='gray'>This exercise is for self-study and will not be discussed in class.\n",
    "\n",
    ">  <font color='teal'>\n",
    "Choose an API (e.g., from the list above or based on your own ideas/research) and try to find out how it works:\n",
    "> - Is it free of charge?\n",
    "> - Does it require authentication (i.e., do you need to crate an account, generate access tokens etc.)?\n",
    "> - What can it do (that might be interesting for a data science project)?\n",
    "> - Where can you find the documentation?\n",
    "> - Is there a Python module that facilitates working with the API?\n",
    "\n",
    ">  <font color='teal'>If there is time (and the API is free): Try to make a request through the API."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
