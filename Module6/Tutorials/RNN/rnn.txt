recurrent neural networks , or rnns ( rumelhart et al . , 1986a ) , are a family of neural networks for processing sequential data . much as a convolutional network is a neural network that is specialized for processing a grid of values such as an image , a recurrent neural network is a neural network that is specialized for processing a sequence of values x ( 1 ) , . . . , x ( tau ) . just as convolutional networks can readily scale to images with large width and height , and some convolutional networks can process images of variable size , recurrent networks can scale to much longer sequences than would be practical for networks without sequence - based specialization . most recurrent networks can also process sequences of variable length . to go from multilayer networks to recurrent networks , we need to take advantage of one of the early ideas found in machine learning and statistical models of the1980s : sharing parameters across different parts of a model . parameter sharing makes it possible to extend and apply the model to examples of different forms ( different lengths , here ) and generalize across them . if we had separate parameters for each value of the time index , we could not generalize to sequence lengths not seen during training , nor share statistical strength across different sequence lengths and across different positions in time . such sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence . for example , consider the two sentences " i went to nepal in 2009 " and " in 2009 , i went to nepal . " if we ask a machine learning model to read each sentence and extract the year in which the narrator went to nepal , we would like it to recognize the year 2009 as the relevant piece of information , whether it appears in the sixth word or in the second word of the sentence . suppose that we trained a feedforward network that processes sentences of fixed length . a traditional fully connected feedforward network would have separate parameters for each input feature , so it would need to learn all the rules of the language separately at each position in the sentence . by comparison , a recurrent neural network shares the same weights across several time steps . a related idea is the use of convolution across a 1 - d temporal sequence . this convolutional approach is the basis for time - delay neural networks ( lang andhinton , 1988 ; waibel et al . , 1989 ; lang et al . , 1990 ) . the convolution operation allows a network to share parameters across time but is shallow . the output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input . the idea of parameter sharing manifests in the application of the same convolution kernel at each time step . recurrent networks share parameters in a different way . each member of the output is a function of the previous members of the output . each member of the output is produced using the same update rule applied to the previous outputs . this recurrent formulation results in the sharing of parameters through a very deep computational graph . for the simplicity of exposition , we refer to rnns as operating on a sequence that contains vectors x ( t ) with the time step index t ranging from 1 to tau . in practice , recurrent networks usually operate on minibatches of such sequences , with a different sequence length tau for each member of the minibatch . we have omitted the minibatch indices to simplify notation . moreover , the time step index need not literally refer to the passage of time in the real world . sometimes it refers only to the position in the sequence . rnns may also be applied in two dimensions across spatial data such as images , and even when applied to data involving time , the network may have connections that go backward in time , provided that the entire sequence is observed before it is provided to the network . this chapter extends the idea of a computational graph to include cycles . these cycles represent the influence of the present value of a variable on its own value at a future time step . such computational graphs allow us to define recurrent neural networks . we then describe many different ways to construct , train , and use recurrent neural networks . for more information on recurrent neural networks than is available in this chapter , we refer the reader to the textbook of graves ( 2012 ) . a computational graph is a way to formalize the structure of a set of computations , such as those involved in mapping inputs and parameters to outputs and loss . please refer to section 6 . 5 . 1 for a general introduction . in this section we explain the idea of unfolding a recursive or recurrent computation into a computational graph that has a repetitive structure , typically corresponding to a chain of events . unfolding this graph results in the sharing of parameters across a deep network structure . for example , consider the classical form of a dynamical system : s ( t ) = f ( s ( t - 1 ) ; theta ) , ( 10 . 1 ) where s ( t ) is called the state of the system . equation 10 . 1 is recurrent because the definition of s at time t refers back to the same definition at time t - 1 . for a finite number of time steps tau , the graph can be unfolded by applying the definition tau - 1 times . for example , if we unfold equation 10 . 1 for tau = 3 time steps , we obtains ( 3 ) = f ( s ( 2 ) ; theta ) ( 10 . 2 ) = f ( f ( s ( 1 ) ; theta ) ; theta ) . ( 10 . 3 ) unfolding the equation by repeatedly applying the definition in this way has yielded an expression that does not involve recurrence . such an expression can now be represented by a traditional directed acyclic computational graph . the unfolded computational graph of equation 10 . 1 and equation 10 . 3 is illustrated in figure 10 . 1 . as another example , let us consider a dynamical system driven by an external signal x ( t ) , s ( t ) = f ( s ( t - 1 ) , x ( t ) ; theta ) , ( 10 . 4 ) where we see that the state now contains information about the whole past sequence . recurrent neural networks can be built in many different ways . much as almost any function can be considered a feed forward neural network , essentially any function involving recurrence can be considered a recurrent neural network . many recurrent neural networks use equation 10 . 5 or a similar equation to define the values of their hidden units . to indicate that the state is the hidden units of the network , we now rewrite equation 10 . 4 using the variable h to represent the state , h ( t ) = f ( h ( t - 1 ) , x ( t ) ; theta ) , ( 10 . 5 ) illustrated in figure 10 . 2 ; typical rnns will add extra architectural features such as output layers that read information out of the state h to make predictions . when the recurrent network is trained to perform a task that requires predicting the future from the past , the network typically learns to use h ( t ) as a kind of lossy summary of the task - relevant aspects of the past sequence of inputs up tot . this summary is in general necessarily lossy , since it maps an arbitrary length sequence ( x ( t ) , x ( t - 1 ) , x ( t - 2 ) , . . . , x ( 2 ) , x ( 1 ) ) to a fixed length vector h ( t ) . depending on the training criterion , this summary might selectively keep some aspects of the past sequence with more precision than other aspects . for example , if the rnn is used in statistical language modeling , typically to predict the next word given previous words , storing all the information in the input sequence up to time t may not be necessary ; storing only enough information to predict the rest of the sentence is sufficient . the most demanding situation is when we ask h ( t ) to be rich enough to allow one to approximately recover the input sequence , as in autoencoder frameworks ( chapter 14 ) . equation 10 . 5 can be drawn in two different ways . one way to draw the rnn is with a diagram containing one node for every component that might exist in a physical implementation of the model , such as a biological neural network . in this view , the network defines a circuit that operates in real time , with physical parts whose current state can influence their future state , as in the left of figure 10 . 2 . throughout this chapter , we use a black square in a circuit diagram to indicate that an interaction takes place with a delay of a single time step , from the state at time t to the state at time t + 1 . the other way to draw the rnn is as an unfolded computational graph , in which each component is represented by many different variables , with one variable per time step , representing the state of the component at that point in time . each variable for each time step is drawn as a separate node of the computational graph , as in the right of figure 10 . 2 . what we call unfolding is the operation that maps a circuit , as in the left side of the figure , to a computational graph with repeated pieces , as in the right side . the unfolded graph now has a size that depends on the sequence length . we can represent the unfolded recurrence after t steps with a function g ( t ) : h ( t ) = g ( t ) ( x ( t ) , x ( t - 1 ) , x ( t - 2 ) , . . . , x ( 2 ) , x ( 1 ) ) ( 10 . 6 ) = f ( h ( t - 1 ) , x ( t ) ; theta ) . ( 10 . 7 ) the function g ( t ) takes the whole past sequence ( x ( t ) , x ( t - 1 ) , x ( t - 2 ) , . . . , x ( 2 ) , x ( 1 ) ) as input and produces the current state , but the unfolded recurrent structure allows us to factorize g ( t ) into repeated application of a function f . the unfolding process thus introduces two major advantages : 1 . regardless of the sequence length , the learned model always has the same input size , because it is specified in terms of transition from one state to another state , rather than specified in terms of a variable - length history of states . 2 . it is possible to use the same transition function f with the same parameters at every time step . these two factors make it possible to learn a single model f that operates on all time steps and all sequence lengths , rather than needing to learn a separate model g ( t ) for all possible time steps . learning a single shared model allows generalization to sequence lengths that did not appear in the training set , and enables the model to be estimated with far fewer training examples than would be required without parameter sharing . 